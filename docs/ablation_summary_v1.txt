Ablation Summary (v1)

This document summarizes a set of controlled ablation experiments designed to isolate which factors materially affect the learned decision geometry in sentiment classification on Financial PhraseBank.

The goal of these ablations is not metric maximization, but causal attribution: identifying which components of the training setup are responsible for the emergence or absence of a neutral-dominant failure mode.

All experiments use the same backbone architecture (DeBERTa‑v3‑base), optimizer (AdamW), learning‑rate schedule, and training protocol. Each ablation varies exactly one factor at a time.


1. Training Data Agreement (Primary Ablation)

Setup:
- Model A (anchor): trained on sentences_75agree, with reweighting, no smoothing
- Model B (contrast): trained on sentences_allagree
- add label smoothing, 0.1
- No class reweighting
- Identical hyperparameters and stopping criteria

Evaluation:
- Post‑hoc evaluation on 75 / 66 / 50 / all‑agree validation splits

On Switching Data Agreement:

Across all evaluation splits, both models exhibit strong diagonal dominance and a stable polarity structure: direct negative ↔ positive confusions are essentially absent. Errors concentrate on transitions involving the neutral class.

Model B (trained on all‑agree) shows a sharp agreement‑dependent collapse pattern. On all‑agree and 75‑agree evaluation, off‑diagonal mass is minimal. As evaluation agreement relaxes (66 → 50), misclassified samples increasingly fall into the neutral column, indicating that uncertain boundary cases are systematically absorbed by the neutral class. The dominant failure mode is positive → neutral, followed by negative → neutral.

Model A (trained on 75‑agree) displays a softer decision geometry. Even on all‑agree evaluation, small but structured neutral ↔ positive and negative ↔ neutral confusions are present, indicating less reliance on a hard neutral basin. As agreement decreases, the growth of neutral absorption is smoother and slower than in Model B, and a larger fraction of positive samples remains correctly classified under low‑agreement evaluation.

Interpretation at the geometry level: training agreement primarily controls the curvature of the decision boundary under label noise. Higher‑agreement training yields sharper boundaries with higher peak consistency but stronger collapse toward neutral under distributional noise. Lower‑agreement training sacrifices boundary sharpness on clean data in exchange for a flatter, more stable error surface as annotation agreement degrades.


On adding 0.1 label smoothing:

Error Characteristics under pb75‑agree + reweight + smoothing:

A closer inspection of misclassified samples under the pb75‑agree training regime with both class reweighting and label smoothing reveals a qualitatively distinct error profile compared to high‑agreement or non‑smoothed baselines. In this setting, errors are dominated by low‑ to moderate‑margin decisions rather than high‑confidence mispredictions. The distribution of prediction margins spans a wide range and does not collapse toward saturation, indicating that most mistakes occur near the decision boundary rather than reflecting overconfident belief in an incorrect class.

Error directions remain strongly structured: the dominant channels are neutral ↔ positive in both directions, while negative ↔ positive confusions are almost entirely absent. This confirms that polarity structure is preserved and that uncertainty is expressed through controlled traversal of the neutral boundary rather than polarity inversion. Compared to non‑smoothed models, neutral no longer functions as a terminal absorbing state; instead, misclassifications exhibit approximately symmetric flow between neutral and positive as agreement decreases.

At the sample level, many errors correspond to semantically ambiguous or forward‑looking statements (e.g., guidance, expectations, factual business arrangements) that are known to exhibit annotation disagreement in PhraseBank. Crucially, these samples are not associated with extreme prediction confidence, supporting the interpretation that the observed errors arise from intrinsic label ambiguity rather than model miscalibration or representational failure. This error profile is consistent with a stable “symmetric buffer” regime in which uncertainty is handled elastically rather than collapsed into neutral.

Representative Error Examples:

To concretize the error profile described above, we highlight several representative misclassified samples drawn from the pb75‑agree + reweight + smoothing setting. These examples illustrate that most errors arise from semantically ambiguous or forward‑looking statements rather than polarity reversal.

1) Forward‑looking operational statements (positive → neutral):
   - “Deliveries have started and the network will be ready for a launch soon.”
   - “However, he expects banks to provide alternative financing.”

   These sentences describe anticipated or preparatory actions without realized financial outcomes. While often annotated as positive, they lack explicit payoff signals, placing them near the neutral–positive boundary.

2) Factual business arrangements without affective framing (positive → neutral):
   - “The company will also be compensated for acting as a reserve batch plant.”
   - “The energy‑efficient data center will be built by combining an effective energy solution with state‑of‑the‑art technology.”

   Such statements encode objective business facts. Model preference for neutral reflects a learned distinction between factual description and sentiment expression.

3) Reported speech or promotional language (neutral → positive):
   - “It is a testament to the quality of our LTE solution and our commitment to the Japanese market,” he added.

   Quoted evaluations introduce subjective praise but remain detached from concrete outcomes, leading to bidirectional confusion between neutral and positive.

4) Cost or process‑related disclosures (neutral → negative):
   - “The bank also expects additional costs, related to the Dash 8‑Q400 jets, which the group shelved in October 2007.”

   These cases involve negative facts presented in a report‑like tone, producing weak negative signals that remain close to the neutral boundary.

Across these examples, misclassifications are associated with moderate prediction margins rather than extreme confidence, reinforcing the interpretation that errors reflect intrinsic label ambiguity in PhraseBank rather than model miscalibration or polarity collapse.

With label smoothing enabled (ε = 0.1), the high‑agreement evaluation (all‑agree) remains perfectly separable: all three classes are predicted without error, indicating that smoothing does not compromise peak discriminative capacity on clean labels.

On the in‑distribution 75‑agree evaluation, total error increases slightly, but the error mass is more evenly distributed across off‑diagonal entries. Neutral ↔ positive and negative ↔ neutral confusions are both present at low, comparable magnitudes, with no dominant failure channel.

As evaluation agreement decreases (66 → 50), smoothing produces a systematic change in error flow. Compared to the non‑smoothed 75‑agree baseline, positive → neutral misclassification grows more slowly, while neutral → positive misclassification increases markedly. At 50‑agree, neutral → positive errors exceed positive → neutral errors, indicating that neutral acts as a bidirectional buffer rather than a terminal sink.

Critically, across all evaluation splits, direct negative ↔ positive confusions remain essentially zero. Label smoothing therefore reshapes uncertainty handling without inducing polarity collapse.

Geometrically, label smoothing reduces the stickiness of the neutral basin: uncertain samples are less likely to be permanently absorbed into neutral and more likely to traverse the neutral boundary symmetrically. This results in a flatter, more elastic decision surface under label disagreement, while preserving sharp class separation under high‑agreement conditions.


On Switching off reweighting:

With class reweighting removed, the model retains strong diagonal dominance and preserves the global polarity structure across all evaluation splits. Direct negative ↔ positive confusions remain rare and never become a dominant failure mode.

On the high‑agreement evaluation (all‑agree), performance remains near‑perfect, but a new qualitative signal appears: a small number of neutral and positive samples are misclassified as negative. This direction of error is largely absent in the reweighted baseline and reflects a shift in effective class priors rather than an increase in label noise sensitivity.

On the in‑distribution 75‑agree evaluation, neutral ↔ positive confusions increase relative to the baseline. Both positive → neutral and neutral → positive channels grow, but the increase is asymmetric, with absorption into neutral exceeding neutral‑to‑positive recovery.

As evaluation agreement decreases (66 → 50), this asymmetry becomes more pronounced. Compared to the reweighted baseline, positive → neutral errors grow faster than neutral → positive errors, indicating that the neutral class increasingly functions as a weak sink rather than a symmetric buffer. At 50‑agree, neutral absorption clearly dominates over neutral recovery.

Across all splits, removal of reweighting also introduces a small but consistent leakage of samples toward the negative class (neutral → negative, positive → negative), a pattern not observed in the reweighted baseline.


2. Consolidated Conclusions

Across all ablations, we find that the dominant failure modes of the model are not governed by polarity confusion between negative and positive classes, but by the geometric role assumed by the neutral class under different training protocols.

First, training data agreement primarily controls the curvature of the decision boundary under label disagreement. High-agreement training yields sharper boundaries and higher peak consistency on clean labels, but induces rapid collapse toward neutral when evaluated on lower-agreement data. Moderate-agreement training produces flatter boundaries and a continuous degradation pattern, avoiding abrupt neutral collapse.

Second, label smoothing reshapes uncertainty flow without degrading separability on clean data. Its primary effect is to reduce the stickiness of the neutral basin: positive-to-neutral absorption grows more slowly, while neutral-to-positive recovery increases as agreement decreases. As a result, neutral transitions from an absorbing state to a symmetric buffer under label noise.

Third, class reweighting modulates the effective energy depth of the neutral basin. Removing reweighting does not destroy polarity structure, but it deepens the neutral basin, increases asymmetric absorption of uncertain samples, and introduces mild leakage toward the negative class due to shifted class priors.

Taken together, these results indicate that the neutral class is not a passive category but a structurally induced attractor whose behavior is explicitly shaped by training agreement, label smoothing, and class reweighting. Different combinations of these factors correspond to distinct failure regimes—neutral collapse, weak sink, or symmetric buffer—which are directly observable in confusion-matrix geometry and cannot be inferred from scalar metrics alone.


3. Implications for Scaling and Ablation Closure

Based on the ablation results above, we conclude that further single‑factor ablations are unlikely to yield additional causal insight. The three primary axes—training agreement, label smoothing, and class reweighting—already form a closed explanatory system that accounts for all observed changes in confusion‑matrix geometry. Additional sweeps (e.g., larger smoothing coefficients or alternative weighting schedules) would primarily amplify known effects rather than reveal new failure regimes.

Accordingly, we conclude the ablation phase at this point.

For future large‑scale training and data expansion, we select the model trained on sentences_75agree with both label smoothing (ε = 0.1) and class reweighting as the scaling baseline. This configuration preserves sharp separability on high‑agreement data while maintaining symmetric uncertainty buffering as annotation agreement decreases. Empirically, it avoids both rapid neutral collapse (observed under all‑agree training) and asymmetric neutral absorption (observed when reweighting is removed).

Conceptually, this choice corresponds to a decision geometry with moderate boundary curvature, reduced neutral basin stickiness, and an elevated neutral energy floor. Such a configuration is expected to remain stable as additional, noisier data are introduced, making it a suitable anchor for continued training, domain expansion, or downstream transfer.
